{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Training a Decision Tree or a Random Forest on a classification problem, and compare the latter with using adaBoost\n",
    "\n",
    "**Author: Pr Fabien MOUTARDE, Center for Robotics, MINES ParisTech, PSL Universit√© Paris**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Decision Trees with SciKit-Learn on a very simple dataset\n",
    "\n",
    "**We will first work on very simple classic dataset: Iris, which is a classification problem corresponding to determination of iris flower sub-species based on a few geometric characteristics of the flower.**\n",
    "\n",
    "**Please FIRST READ the [*Iris DATASET DESCRIPTION*](http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html#sphx-glr-auto-examples-datasets-plot-iris-dataset-py).**\n",
    "In this classification problem, there are 3 classes, with a total of 150 examples (each one with 4 input). Please **now execute code cell below to load and view the dataset**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Number_of_examples, example_size) =  (150, 4) \n",
      "\n",
      "Input =  [5.1 3.5 1.4 0.2]  , Label =  0\n",
      "Input =  [4.9 3.  1.4 0.2]  , Label =  0\n",
      "Input =  [4.7 3.2 1.3 0.2]  , Label =  0\n",
      "Input =  [4.6 3.1 1.5 0.2]  , Label =  0\n",
      "Input =  [5.  3.6 1.4 0.2]  , Label =  0\n",
      "Input =  [5.4 3.9 1.7 0.4]  , Label =  0\n",
      "Input =  [4.6 3.4 1.4 0.3]  , Label =  0\n",
      "Input =  [5.  3.4 1.5 0.2]  , Label =  0\n",
      "Input =  [4.4 2.9 1.4 0.2]  , Label =  0\n",
      "Input =  [4.9 3.1 1.5 0.1]  , Label =  0\n",
      "Input =  [5.4 3.7 1.5 0.2]  , Label =  0\n",
      "Input =  [4.8 3.4 1.6 0.2]  , Label =  0\n",
      "Input =  [4.8 3.  1.4 0.1]  , Label =  0\n",
      "Input =  [4.3 3.  1.1 0.1]  , Label =  0\n",
      "Input =  [5.8 4.  1.2 0.2]  , Label =  0\n",
      "Input =  [5.7 4.4 1.5 0.4]  , Label =  0\n",
      "Input =  [5.4 3.9 1.3 0.4]  , Label =  0\n",
      "Input =  [5.1 3.5 1.4 0.3]  , Label =  0\n",
      "Input =  [5.7 3.8 1.7 0.3]  , Label =  0\n",
      "Input =  [5.1 3.8 1.5 0.3]  , Label =  0\n",
      "Input =  [5.4 3.4 1.7 0.2]  , Label =  0\n",
      "Input =  [5.1 3.7 1.5 0.4]  , Label =  0\n",
      "Input =  [4.6 3.6 1.  0.2]  , Label =  0\n",
      "Input =  [5.1 3.3 1.7 0.5]  , Label =  0\n",
      "Input =  [4.8 3.4 1.9 0.2]  , Label =  0\n",
      "Input =  [5.  3.  1.6 0.2]  , Label =  0\n",
      "Input =  [5.  3.4 1.6 0.4]  , Label =  0\n",
      "Input =  [5.2 3.5 1.5 0.2]  , Label =  0\n",
      "Input =  [5.2 3.4 1.4 0.2]  , Label =  0\n",
      "Input =  [4.7 3.2 1.6 0.2]  , Label =  0\n",
      "Input =  [4.8 3.1 1.6 0.2]  , Label =  0\n",
      "Input =  [5.4 3.4 1.5 0.4]  , Label =  0\n",
      "Input =  [5.2 4.1 1.5 0.1]  , Label =  0\n",
      "Input =  [5.5 4.2 1.4 0.2]  , Label =  0\n",
      "Input =  [4.9 3.1 1.5 0.2]  , Label =  0\n",
      "Input =  [5.  3.2 1.2 0.2]  , Label =  0\n",
      "Input =  [5.5 3.5 1.3 0.2]  , Label =  0\n",
      "Input =  [4.9 3.6 1.4 0.1]  , Label =  0\n",
      "Input =  [4.4 3.  1.3 0.2]  , Label =  0\n",
      "Input =  [5.1 3.4 1.5 0.2]  , Label =  0\n",
      "Input =  [5.  3.5 1.3 0.3]  , Label =  0\n",
      "Input =  [4.5 2.3 1.3 0.3]  , Label =  0\n",
      "Input =  [4.4 3.2 1.3 0.2]  , Label =  0\n",
      "Input =  [5.  3.5 1.6 0.6]  , Label =  0\n",
      "Input =  [5.1 3.8 1.9 0.4]  , Label =  0\n",
      "Input =  [4.8 3.  1.4 0.3]  , Label =  0\n",
      "Input =  [5.1 3.8 1.6 0.2]  , Label =  0\n",
      "Input =  [4.6 3.2 1.4 0.2]  , Label =  0\n",
      "Input =  [5.3 3.7 1.5 0.2]  , Label =  0\n",
      "Input =  [5.  3.3 1.4 0.2]  , Label =  0\n",
      "Input =  [7.  3.2 4.7 1.4]  , Label =  1\n",
      "Input =  [6.4 3.2 4.5 1.5]  , Label =  1\n",
      "Input =  [6.9 3.1 4.9 1.5]  , Label =  1\n",
      "Input =  [5.5 2.3 4.  1.3]  , Label =  1\n",
      "Input =  [6.5 2.8 4.6 1.5]  , Label =  1\n",
      "Input =  [5.7 2.8 4.5 1.3]  , Label =  1\n",
      "Input =  [6.3 3.3 4.7 1.6]  , Label =  1\n",
      "Input =  [4.9 2.4 3.3 1. ]  , Label =  1\n",
      "Input =  [6.6 2.9 4.6 1.3]  , Label =  1\n",
      "Input =  [5.2 2.7 3.9 1.4]  , Label =  1\n",
      "Input =  [5.  2.  3.5 1. ]  , Label =  1\n",
      "Input =  [5.9 3.  4.2 1.5]  , Label =  1\n",
      "Input =  [6.  2.2 4.  1. ]  , Label =  1\n",
      "Input =  [6.1 2.9 4.7 1.4]  , Label =  1\n",
      "Input =  [5.6 2.9 3.6 1.3]  , Label =  1\n",
      "Input =  [6.7 3.1 4.4 1.4]  , Label =  1\n",
      "Input =  [5.6 3.  4.5 1.5]  , Label =  1\n",
      "Input =  [5.8 2.7 4.1 1. ]  , Label =  1\n",
      "Input =  [6.2 2.2 4.5 1.5]  , Label =  1\n",
      "Input =  [5.6 2.5 3.9 1.1]  , Label =  1\n",
      "Input =  [5.9 3.2 4.8 1.8]  , Label =  1\n",
      "Input =  [6.1 2.8 4.  1.3]  , Label =  1\n",
      "Input =  [6.3 2.5 4.9 1.5]  , Label =  1\n",
      "Input =  [6.1 2.8 4.7 1.2]  , Label =  1\n",
      "Input =  [6.4 2.9 4.3 1.3]  , Label =  1\n",
      "Input =  [6.6 3.  4.4 1.4]  , Label =  1\n",
      "Input =  [6.8 2.8 4.8 1.4]  , Label =  1\n",
      "Input =  [6.7 3.  5.  1.7]  , Label =  1\n",
      "Input =  [6.  2.9 4.5 1.5]  , Label =  1\n",
      "Input =  [5.7 2.6 3.5 1. ]  , Label =  1\n",
      "Input =  [5.5 2.4 3.8 1.1]  , Label =  1\n",
      "Input =  [5.5 2.4 3.7 1. ]  , Label =  1\n",
      "Input =  [5.8 2.7 3.9 1.2]  , Label =  1\n",
      "Input =  [6.  2.7 5.1 1.6]  , Label =  1\n",
      "Input =  [5.4 3.  4.5 1.5]  , Label =  1\n",
      "Input =  [6.  3.4 4.5 1.6]  , Label =  1\n",
      "Input =  [6.7 3.1 4.7 1.5]  , Label =  1\n",
      "Input =  [6.3 2.3 4.4 1.3]  , Label =  1\n",
      "Input =  [5.6 3.  4.1 1.3]  , Label =  1\n",
      "Input =  [5.5 2.5 4.  1.3]  , Label =  1\n",
      "Input =  [5.5 2.6 4.4 1.2]  , Label =  1\n",
      "Input =  [6.1 3.  4.6 1.4]  , Label =  1\n",
      "Input =  [5.8 2.6 4.  1.2]  , Label =  1\n",
      "Input =  [5.  2.3 3.3 1. ]  , Label =  1\n",
      "Input =  [5.6 2.7 4.2 1.3]  , Label =  1\n",
      "Input =  [5.7 3.  4.2 1.2]  , Label =  1\n",
      "Input =  [5.7 2.9 4.2 1.3]  , Label =  1\n",
      "Input =  [6.2 2.9 4.3 1.3]  , Label =  1\n",
      "Input =  [5.1 2.5 3.  1.1]  , Label =  1\n",
      "Input =  [5.7 2.8 4.1 1.3]  , Label =  1\n",
      "Input =  [6.3 3.3 6.  2.5]  , Label =  2\n",
      "Input =  [5.8 2.7 5.1 1.9]  , Label =  2\n",
      "Input =  [7.1 3.  5.9 2.1]  , Label =  2\n",
      "Input =  [6.3 2.9 5.6 1.8]  , Label =  2\n",
      "Input =  [6.5 3.  5.8 2.2]  , Label =  2\n",
      "Input =  [7.6 3.  6.6 2.1]  , Label =  2\n",
      "Input =  [4.9 2.5 4.5 1.7]  , Label =  2\n",
      "Input =  [7.3 2.9 6.3 1.8]  , Label =  2\n",
      "Input =  [6.7 2.5 5.8 1.8]  , Label =  2\n",
      "Input =  [7.2 3.6 6.1 2.5]  , Label =  2\n",
      "Input =  [6.5 3.2 5.1 2. ]  , Label =  2\n",
      "Input =  [6.4 2.7 5.3 1.9]  , Label =  2\n",
      "Input =  [6.8 3.  5.5 2.1]  , Label =  2\n",
      "Input =  [5.7 2.5 5.  2. ]  , Label =  2\n",
      "Input =  [5.8 2.8 5.1 2.4]  , Label =  2\n",
      "Input =  [6.4 3.2 5.3 2.3]  , Label =  2\n",
      "Input =  [6.5 3.  5.5 1.8]  , Label =  2\n",
      "Input =  [7.7 3.8 6.7 2.2]  , Label =  2\n",
      "Input =  [7.7 2.6 6.9 2.3]  , Label =  2\n",
      "Input =  [6.  2.2 5.  1.5]  , Label =  2\n",
      "Input =  [6.9 3.2 5.7 2.3]  , Label =  2\n",
      "Input =  [5.6 2.8 4.9 2. ]  , Label =  2\n",
      "Input =  [7.7 2.8 6.7 2. ]  , Label =  2\n",
      "Input =  [6.3 2.7 4.9 1.8]  , Label =  2\n",
      "Input =  [6.7 3.3 5.7 2.1]  , Label =  2\n",
      "Input =  [7.2 3.2 6.  1.8]  , Label =  2\n",
      "Input =  [6.2 2.8 4.8 1.8]  , Label =  2\n",
      "Input =  [6.1 3.  4.9 1.8]  , Label =  2\n",
      "Input =  [6.4 2.8 5.6 2.1]  , Label =  2\n",
      "Input =  [7.2 3.  5.8 1.6]  , Label =  2\n",
      "Input =  [7.4 2.8 6.1 1.9]  , Label =  2\n",
      "Input =  [7.9 3.8 6.4 2. ]  , Label =  2\n",
      "Input =  [6.4 2.8 5.6 2.2]  , Label =  2\n",
      "Input =  [6.3 2.8 5.1 1.5]  , Label =  2\n",
      "Input =  [6.1 2.6 5.6 1.4]  , Label =  2\n",
      "Input =  [7.7 3.  6.1 2.3]  , Label =  2\n",
      "Input =  [6.3 3.4 5.6 2.4]  , Label =  2\n",
      "Input =  [6.4 3.1 5.5 1.8]  , Label =  2\n",
      "Input =  [6.  3.  4.8 1.8]  , Label =  2\n",
      "Input =  [6.9 3.1 5.4 2.1]  , Label =  2\n",
      "Input =  [6.7 3.1 5.6 2.4]  , Label =  2\n",
      "Input =  [6.9 3.1 5.1 2.3]  , Label =  2\n",
      "Input =  [5.8 2.7 5.1 1.9]  , Label =  2\n",
      "Input =  [6.8 3.2 5.9 2.3]  , Label =  2\n",
      "Input =  [6.7 3.3 5.7 2.5]  , Label =  2\n",
      "Input =  [6.7 3.  5.2 2.3]  , Label =  2\n",
      "Input =  [6.3 2.5 5.  1.9]  , Label =  2\n",
      "Input =  [6.5 3.  5.2 2. ]  , Label =  2\n",
      "Input =  [6.2 3.4 5.4 2.3]  , Label =  2\n",
      "Input =  [5.9 3.  5.1 1.8]  , Label =  2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn import preprocessing \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load Iris classification dataset\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "# Print all 150 examples\n",
    "print(\"(Number_of_examples, example_size) = \" , iris.data.shape, \"\\n\")\n",
    "for i in range(0, 150) :\n",
    "    print('Input = ', iris.data[i], ' , Label = ', iris.target[i] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building, training and evaluating a simple Decision Tree classifier**\n",
    "\n",
    "The SciKit-learn class for Decision Tree classifiers is sklearn.tree.DecisionTreeClassifier.\n",
    "\n",
    "**Please FIRST READ (and understand!) the [*DecisionTreeClassifier DOCUMENTATION*](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) to understand all parameters of the contructor.**\n",
    "\n",
    "**You can then begin by running the code block below, in which default set of parameter values has been used.** If graphical view works, look at the structure of the learnt decision tree.\n",
    "\n",
    "**Then, check the influence of MAIN parameters for Decision Tree classifier, i.e.:**\n",
    " - **homegeneity criterion ('gini' or 'entropy')**\n",
    " - **max_depth**\n",
    " - **min_samples_split**\n",
    " \n",
    "NB : Note that post-training *PRUNING* IS unfortunately *NOT* implemented in SciKit-Learn Decision-Trees :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=5,\n",
      "                       max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=1e-07,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=None, splitter='best')\n",
      "Acuracy (on test set) =  0.9111111111111111\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        18\n",
      "           1       0.75      1.00      0.86        12\n",
      "           2       1.00      0.73      0.85        15\n",
      "\n",
      "    accuracy                           0.91        45\n",
      "   macro avg       0.92      0.91      0.90        45\n",
      "weighted avg       0.93      0.91      0.91        45\n",
      "\n",
      "\n",
      " CONFUSION MATRIX\n",
      "[[18  0  0]\n",
      " [ 0 12  0]\n",
      " [ 0  4 11]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rpatureau\\Documents\\Winpython\\python-3.7.4.amd64\\lib\\site-packages\\sklearn\\tree\\tree.py:297: DeprecationWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset into training and test part\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3)\n",
    "\n",
    "# Learn a Decision Tree\n",
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy', splitter='best', max_depth=5, \n",
    "                                  min_samples_split=2, min_samples_leaf=1, \n",
    "                                  min_weight_fraction_leaf=0.0, max_features=None, \n",
    "                                  random_state=None, max_leaf_nodes=None, \n",
    "                                  min_impurity_split=1e-07, class_weight=None, presort=False)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "# Graphical view of learnt Decision Tree\n",
    "#\n",
    "#import pydotplus \n",
    "#dot_data = tree.export_graphviz(clf, out_file=None) \n",
    "#graph = pydotplus.graph_from_dot_data(dot_data) \n",
    "#graph.write_pdf(\"iris.pdf\")\n",
    "#from IPython.display import Image \n",
    "#Image(graph.create_png()) \n",
    "\n",
    "# Evaluate acuracy on test data\n",
    "print(clf)\n",
    "score = clf.score(X_test, y_test)\n",
    "print(\"Acuracy (on test set) = \", score)\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print( classification_report(y_true, y_pred) )\n",
    "print(\"\\n CONFUSION MATRIX\")\n",
    "print( confusion_matrix(y_true, y_pred) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Decision Trees on a  MORE REALISTIC DATASET: HANDWRITTEN DIGITS\n",
    "\n",
    "**Please FIRST READ the [*Digits DATASET DESCRIPTION*](http://scikit-learn.org/stable/auto_examples/datasets/plot_digits_last_image.html#sphx-glr-auto-examples-datasets-plot-digits-last-image-py).**\n",
    "\n",
    "In this classification problem, there are 10 classes, with a total of 1797 examples (each one being a 64D vector corresponding to an 8x8 pixmap). Please **now execute code cell below to load the dataset, visualize a typical example, and train a Desicion Tree on it**. \n",
    "The original code uses a **SUBOPTIMAL set of learning hyperparameters values. Try to play with them in order to improve acuracy.**\n",
    "\n",
    "Finally, **find a somewhat optimized setting of the set of 3 main hyper-parameters for Decision Tree learning, by using CROSS-VALIDATION** (see cross-validation example from the Multi-Layer Perceptron notebook used in earlier practical session).\n",
    "\n",
    "Look at final acuracy statistics, and also at the confusion-matrix: what digits are the most confused with each other ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number_of-examples =  1797\n",
      "\n",
      " Plot of first example\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "                       max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=1e-07,\n",
      "                       min_samples_leaf=1, min_samples_split=8,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=None, splitter='best')\n",
      "Acuracy (on test set) =  0.6440489432703004\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.93        87\n",
      "           1       0.50      0.22      0.30        87\n",
      "           2       0.50      0.17      0.26        86\n",
      "           3       0.42      0.79      0.55        87\n",
      "           4       0.84      0.82      0.83        89\n",
      "           5       0.99      0.78      0.87        96\n",
      "           6       0.87      0.93      0.90        94\n",
      "           7       0.90      0.67      0.76        90\n",
      "           8       0.33      0.80      0.47        91\n",
      "           9       0.96      0.27      0.42        92\n",
      "\n",
      "    accuracy                           0.64       899\n",
      "   macro avg       0.72      0.64      0.63       899\n",
      "weighted avg       0.73      0.64      0.63       899\n",
      "\n",
      "\n",
      " CONFUSION MATRIX\n",
      "[[83  0  0  0  1  0  0  0  3  0]\n",
      " [ 0 19  4 14  3  0  3  0 44  0]\n",
      " [ 0  3 15  5  0  0  2  1 60  0]\n",
      " [ 0  3  5 69  0  1  0  0  9  0]\n",
      " [ 0  7  0  0 73  0  3  2  4  0]\n",
      " [ 5  0  1  5  3 75  5  0  2  0]\n",
      " [ 0  4  0  0  2  0 87  0  1  0]\n",
      " [ 0  1  1  2  4  0  0 60 21  1]\n",
      " [ 2  1  3 12  0  0  0  0 73  0]\n",
      " [ 2  0  1 57  1  0  0  4  2 25]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rpatureau\\Documents\\Winpython\\python-3.7.4.amd64\\lib\\site-packages\\sklearn\\tree\\tree.py:297: DeprecationWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "n_samples = len(digits.images)\n",
    "print(\"Number_of-examples = \", n_samples)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"\\n Plot of first example\")\n",
    "plt.gray() \n",
    "plt.matshow(digits.images[0]) \n",
    "plt.show() \n",
    "\n",
    "# Flatten the images, to turn data in a (samples, feature) matrix:\n",
    "data = digits.images.reshape((n_samples, -1))\n",
    "\n",
    "# Split dataset into training and test part\n",
    "X = data\n",
    "y = digits.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n",
    "\n",
    "# Create and train a Decision Tree Classifier\n",
    "clf = tree.DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=5, \n",
    "                                  min_samples_split=8, min_samples_leaf=1, \n",
    "                                  min_weight_fraction_leaf=0.0, max_features=None, \n",
    "                                  random_state=None, max_leaf_nodes=None, \n",
    "                                  min_impurity_split=1e-07, class_weight=None, presort=False)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate acuracy on test data\n",
    "print(clf)\n",
    "score = clf.score(X_test, y_test)\n",
    "print(\"Acuracy (on test set) = \", score)\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print( classification_report(y_true, y_pred) )\n",
    "print(\"\\n CONFUSION MATRIX\")\n",
    "print( confusion_matrix(y_true, y_pred) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building, training and evaluating a Random Forest classifier\n",
    "\n",
    "The SciKit-learn class for Random Forest classifiers is Please sklearn.ensemble.RandomForestClassifier.\n",
    "\n",
    "**Please FIRST READ (and understand!) the [*RandomForestClassifier DOCUMENTATION*](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) to understand all parameters of the contructor.**\n",
    "\n",
    "**Then you can begin by running the code block below, in which default set of parameter values has been used.** As you will see, a RandomForest (even rather small) can easily outperform single Decision Tree. \n",
    "\n",
    "**Then, check the influence of MAIN parameters for Random Forest classifier, i.e.:**\n",
    " - **n_estimators (number of trees in forest)**\n",
    " - **max_depth**\n",
    " - **max_features (max number of features used in each tree)**\n",
    "\n",
    "**Finally, find a somewhat optimized setting of the above set of 3 main parameters, by using CROSS-VALIDATION.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators= 10  max_depth= None max_features= auto\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=1e-07,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "                       oob_score=False, random_state=None, verbose=0,\n",
      "                       warm_start=False)\n",
      "Acuracy (on test set) =  0.9276974416017798\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.97        87\n",
      "           1       0.83      0.97      0.89        89\n",
      "           2       0.93      0.96      0.95        84\n",
      "           3       0.92      0.83      0.87        92\n",
      "           4       0.98      0.94      0.96        88\n",
      "           5       0.92      0.96      0.94        94\n",
      "           6       1.00      0.98      0.99        94\n",
      "           7       0.94      0.94      0.94        99\n",
      "           8       0.93      0.77      0.84        84\n",
      "           9       0.89      0.94      0.92        88\n",
      "\n",
      "    accuracy                           0.93       899\n",
      "   macro avg       0.93      0.93      0.93       899\n",
      "weighted avg       0.93      0.93      0.93       899\n",
      "\n",
      "\n",
      " CONFUSION MATRIX\n",
      "[[85  0  0  0  2  0  0  0  0  0]\n",
      " [ 0 86  0  0  0  1  0  1  0  1]\n",
      " [ 0  3 81  0  0  0  0  0  0  0]\n",
      " [ 2  0  5 76  0  5  0  0  3  1]\n",
      " [ 0  2  0  0 83  0  0  2  0  1]\n",
      " [ 0  0  0  1  0 90  0  0  0  3]\n",
      " [ 0  1  0  0  0  0 92  0  1  0]\n",
      " [ 0  0  1  1  0  0  0 93  1  3]\n",
      " [ 1 11  0  2  0  1  0  3 65  1]\n",
      " [ 0  1  0  3  0  1  0  0  0 83]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rpatureau\\Documents\\Winpython\\python-3.7.4.amd64\\lib\\site-packages\\sklearn\\tree\\tree.py:297: DeprecationWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\rpatureau\\Documents\\Winpython\\python-3.7.4.amd64\\lib\\site-packages\\sklearn\\tree\\tree.py:297: DeprecationWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\rpatureau\\Documents\\Winpython\\python-3.7.4.amd64\\lib\\site-packages\\sklearn\\tree\\tree.py:297: DeprecationWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\rpatureau\\Documents\\Winpython\\python-3.7.4.amd64\\lib\\site-packages\\sklearn\\tree\\tree.py:297: DeprecationWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\rpatureau\\Documents\\Winpython\\python-3.7.4.amd64\\lib\\site-packages\\sklearn\\tree\\tree.py:297: DeprecationWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\rpatureau\\Documents\\Winpython\\python-3.7.4.amd64\\lib\\site-packages\\sklearn\\tree\\tree.py:297: DeprecationWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\rpatureau\\Documents\\Winpython\\python-3.7.4.amd64\\lib\\site-packages\\sklearn\\tree\\tree.py:297: DeprecationWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\rpatureau\\Documents\\Winpython\\python-3.7.4.amd64\\lib\\site-packages\\sklearn\\tree\\tree.py:297: DeprecationWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\rpatureau\\Documents\\Winpython\\python-3.7.4.amd64\\lib\\site-packages\\sklearn\\tree\\tree.py:297: DeprecationWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\rpatureau\\Documents\\Winpython\\python-3.7.4.amd64\\lib\\site-packages\\sklearn\\tree\\tree.py:297: DeprecationWarning: The min_impurity_split parameter is deprecated. Its default value will change from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create and train a Random Forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=10, criterion='gini', max_depth=None,\n",
    "                             min_samples_split=2, min_samples_leaf=1, \n",
    "                             min_weight_fraction_leaf=0.0, max_features='auto', \n",
    "                             max_leaf_nodes=None, min_impurity_split=1e-07, bootstrap=True, \n",
    "                             oob_score=False, n_jobs=1, random_state=None, \n",
    "                             verbose=0, warm_start=False, class_weight=None)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "print(\"n_estimators=\", clf.n_estimators, \" max_depth=\",clf.max_depth,\n",
    "      \"max_features=\", clf.max_features)\n",
    "\n",
    "# Evaluate acuracy on test data\n",
    "print(clf)\n",
    "score = clf.score(X_test, y_test)\n",
    "print(\"Acuracy (on test set) = \", score)\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print( classification_report(y_true, y_pred) )\n",
    "print(\"\\n CONFUSION MATRIX\")\n",
    "print( confusion_matrix(y_true, y_pred) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building, training and evaluating an AdaBoost classifier\n",
    "\n",
    "The SciKit-learn class for adaBoost is sklearn.ensemble.AdaBoostClassifier.\n",
    "\n",
    "**Please FIRST READ (and understand!) the [*AdaBoostClassifier DOCUMENTATION*](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier) to understand all parameters of the contructor.**\n",
    "\n",
    "**Then begin by running the code block below, in which a default set of parameter values has been used.** Look at the training curve: you can see that **training error goes down to zero rather quickly, and that test_error continues to diminish with increasing iterations**.\n",
    "\n",
    "**Then, check the influence of MAIN parameters for adaBoost classifier, i.e.:**\n",
    " - **base_estimator (ie type of Weak Classifier/Learner)** \n",
    " - **n_estimators (number of boosting iterations, and therefore also number of weak classifiers)**\n",
    " - algorithm\n",
    "\n",
    "**First, for the case of DecisionTree weak classifiers, find somewhat optimized values of (max_depth, n_estimators) by using CROSS-VALIDATION.**\n",
    "\n",
    "**Finally, check which other types of classifiers can be used as Weak Classifier with the adaBoost implementation of SciKit-Learn.**\n",
    "\n",
    "NB: in principle it is possible to use MLP classifiers as weak classifiers, but not with SciKit-learn implementation of MLPClassifier (because weighting of examples is not handled)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weak_learner: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=6,\n",
      "                       max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=None, splitter='best')\n",
      "Weights of weak classifiers:  [4.27416299 5.09147342 4.52318099 4.87748784 5.6613437  5.9459768\n",
      " 4.34357778 4.80747334 5.24448217 4.34385511 4.89723856 5.41247739\n",
      " 5.54241232 4.69001613 6.03575332]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deXxU9b3/8dcnk4QECGvYApEl7CKbgAoRVJaE2CvUYutCtWov0gpVW3vr/V2vS5db2mvVFlBERGvrLW61UiogKgKu7PsiAVkiW0C2hCXb5/fHOcFJmCQzyUxmkvk8eZzHzJw5yycB5j3nnO/5fkVVMcYYE71iwl2AMcaY8LIgMMaYKGdBYIwxUc6CwBhjopwFgTHGRDkLAmOMiXIhDQIRyRSRHSKSLSIP+Xj/GhE5KSLr3emRUNZjjDHmYrGh2rCIeICZwGggB1glIvNVdWu5RVeo6rdCVYcxxpjKhfKIYAiQraq7VbUAmAeMC+H+jDHGVEPIjgiA9sB+r9c5wBU+lrtKRDYAB4AHVXVL+QVEZBIwCaBRo0aX9+zZMwTlGmNM/bVmzZqjqtrK13uhDALxMa98fxZrgY6qmiciWcA/gG4XraQ6G5gNMGjQIF29enWwazXGmHpNRPZW9F4oTw3lAKlerzvgfOu/QFVPqWqe+/wdIE5EkkNYkzHGmHJCGQSrgG4i0llE4oGbgfneC4hIWxER9/kQt55jIazJGGNMOSE7NaSqRSIyBVgMeIC5qrpFRCa7788CJgA/EpEi4Cxws1p3qMYYU6ukrn3u2jUCY8KrsLCQnJwczp07F+5SjA8JCQl06NCBuLi4MvNFZI2qDvK1TigvFhtj6qGcnBySkpLo1KkT7pldEyFUlWPHjpGTk0Pnzp39Xs+6mDDGBOTcuXO0bNnSQiACiQgtW7YM+GjNgsAYEzALgchVnb8bCwJjjIlydo3AGFOnHDt2jJEjRwJw6NAhPB4PrVo5N8yuXLmS+Pj4Krdx55138tBDD9GjR48Kl5k5cybNmjXjtttuq3HN6enp5ObmkpiYCECPHj149dVXa7zdYLEgMMbUKS1btmT9+vUAPPbYYzRu3JgHH3ywzDKqiqoSE+P7pMeLL75Y5X7uvffemhfr5dVXX6V///4Vvl9UVERsbGyFr/1drzosCIwx9UJ2djbjx48nPT2dzz//nAULFvD444+zdu1azp49y/e+9z0eecTp6T49PZ0ZM2bQp08fkpOTmTx5MgsXLqRhw4a8/fbbtG7dmocffpjk5GTuv/9+0tPTSU9P54MPPuDkyZO8+OKLDB06lPz8fG6//Xays7Pp3bs3O3fuZM6cOZV+4HubOHEibdq0Ye3atQwePJj4+Hhyc3PZvXs3bdu2Zfbs2UyePJm1a9cSFxfH008/zfDhw5kzZw7vvfceeXl5nD9/niVLltTod2dBYIyptvsX3c/6Q+uDus3+bfvzdObT1Vp369atvPjii8yaNQuAadOm0aJFC4qKirj22muZMGECvXv3LrPOyZMnGTFiBNOmTeOnP/0pc+fO5aGHLho+BVVl5cqVzJ8/n1/+8pcsWrSI6dOn07ZtW9588002bNjAwIEDK6zte9/73oVTQ5mZmUybNg2AXbt28f777xMTE8PDDz/MunXrWL58OQkJCfzud78jPj6eTZs2sWXLFrKysti5cycAn376KevXr6d58+bV+l15syAwxtQbaWlpDB48+MLrv/3tb7zwwgsUFRVx4MABtm7delEQJCYmMnbsWAAuv/xyVqxY4XPbN95444Vl9uzZA8BHH33EL37xCwD69evHpZdeWmFtFZ0auummm8qcwho3bhwJCQkXtv/zn/8cgEsvvZSUlBSys7MBGDNmTFBCACwIjDE1UN1v7qHSqFGjC8937tzJH//4R1auXEmzZs2YOHGiz/b13heXPR4PRUVFPrfdoEGDi5YJRs8M3jWXf13Z9suvVxPWfNQYUy+dOnWKpKQkmjRpwsGDB1m8eHHQ95Gens5rr70GwKZNm9i6tfwAjDUzfPhwXnnlFQC2bdvGwYMH6dq1a1D3AXZEYIyppwYOHEjv3r3p06cPXbp0YdiwYUHfx9SpU7n99tvp27cvAwcOpE+fPjRt2tTnst7XCNq0aeNXME2dOpV77rmHyy67jLi4OF5++WW/mscGyjqdM8YEZNu2bfTq1SvcZUSEoqIiioqKSEhIYOfOnYwZM4adO3fWuDlnTfn6O7JO54wxJgTy8vIYOXIkRUVFqCrPPfdc2EOgOupexcYYEyGaNWvGmjVrwl1GjdnFYmOMiXIWBMYYE+UsCIwxJspZEBhjTJSzi8XGmDolGN1QA8ydO5esrCzatm170XsTJ07k448/vnBPQFJSUoVdT9QHFgTGmDrFn26o/TF37lwGDhzoMwgAnnrqKcaPH1/h+uHsNjrYIqsaY4ypgT//+c/MnDmTgoIChg4dyowZMygpKeHOO+9k/fr1qCqTJk2iTZs2rF+//sLdvv4eSTz88MNluokeMWJEme6gFy9ezIMPPsi7776LiPDoo48yYcIE3nvvPaZNm0ZycjJbtmxh06ZNtfDb8J8FgTGm+tbcD8eD2w01zfvD5YF3Zrd582beeustPvnkE2JjY5k0aRLz5s0jLS2No0ePXvjwPXHiBM2aNWP69OnMmDGjwrEDHnjgAR577DEA+vbty8svvwxQppvoOXPmlOkO+tVXX2Xr1q1s2LCB3NxcBg8ezPDhwwH47LPP2Lp1K5dcckk1fimhZUFgjKkX3nvvPVatWsWgQU4vCmfPniU1NZWMjAx27NjBfffdR1ZWFmPGjPFrexWdGvLuJhrKdgf90Ucfceutt+LxeGjbti3p6emsXr2a+Ph4rrrqqogMAbAgMMbURDW+uYeKqnLXXXfxq1/96qL3Nm7cyMKFC/nTn/7Em2++yezZs6u9n0joNjrYrPmoMaZeGDVqFK+99hpHjx4FnNZF+/btIzc3F1XlpptuujB0JTgtgU6fPh3UGoYPH868efMoLi7m8OHDfPzxxxeOUCKZHREYY+qFyy67jEcffZRRo0ZRUlJCXFwcs2bNwuPxcPfdd6OqiAi/+93vALjzzjv54Q9/WOHFYu9rBIBffQpNmDCBzz77jH79+iEiPPnkk7Ru3TqoP2coWDfUxpiAWDfUkS/Qbqjt1JAxxkQ5CwJjjIlyFgTGmIDVtVPK0aQ6fzcWBMaYgCQkJHDs2DELgwikqhw7dqzMfQ7+sFZDxpiAdOjQgZycHHJzc8NdivEhISGBDh06BLSOBYExJiBxcXF07tw53GWYILJTQ8YYE+VCGgQikikiO0QkW0QeqmS5wSJSLCITQlmPMcaYi4UsCETEA8wExgK9gVtEpHcFy/0OWByqWowxxlQslEcEQ4BsVd2tqgXAPGCcj+WmAm8CR0JYizHGmAqEMgjaA/u9Xue48y4QkfbAt4FZlW1IRCaJyGoRWW0tFYwxJrhCGQTiY175hsdPA79Q1eLKNqSqs1V1kKoOKh2b1BhjTHCEsvloDpDq9boDcKDcMoOAeSICkAxkiUiRqv4jhHUZY4zxEsogWAV0E5HOwFfAzcCt3guo6oXGyCLyErDAQsAYY2pXyIJAVYtEZApOayAPMFdVt4jIZPf9Sq8LGGOMqR0hvbNYVd8B3ik3z2cAqOoPQlmLMcYY3+zOYmOMiXIWBMYYE+UsCIwxJspZEBhjTJSzIDDGmChnQWCMMVHOgsAYY6KcBYExxkQ5CwJjjIlyFgTGGBPlLAiMMSbKWRAYY0yUsyAwxpgoZ0FgjDFRzoLAGGOinAWBMcZEOQsCY4yJchYExhgT5SwIjDEmylkQ1NSWafD+KFANdyXGGFMtIR28vt4rOAlb/geKTsPpndCke7grMsaYgNkRQU3smuOEAMCBd8JbizHGVJMFQXWVFMKOp6H1NdCklwWBMabOsiCorr2vwpkc6PUgtL8ejiyDwrxwV2WMMQGzIKgOVdj2BDTtDSljISULSgrg8PvhrswYYwJmQVAdh9+HExug589AYiB5GMQm2ekhY0ydZEFQHduegIQ20Ok257UnHtqNdoLAmpEaY+oYC4JAHd8IBxdDj5+Ap8E381Oud64ZnNgUvtqMMaYaLAgCtf0PENsIuk4uO79dpvNop4eMMXWMBUEgzuTAnv+DLndDgxZl32uYAs0HWBAYY+ocC4JA7JgOlEDP+32/n5IFRz+BguO1WpYxxtSEBYG/Ck9B9ixInQCNO/teJiULtBgOLqnd2owxpgYsCPy16wUnDHo9WPEyLa+A+BZw4F+1V5cxxtSQBYE/Sgph+1PQegS0HFzxcjEe56LxgYWgJbVXnzHG1EBIg0BEMkVkh4hki8hDPt4fJyIbRWS9iKwWkfRQ1lNt+16HM/srPxoolZIF53Ph6zWhr8sYY4IgZEEgIh5gJjAW6A3cIiK9yy32PtBPVfsDdwFzQlVPtZV2J9Gkp/MhX5V2GYBY6yFjTJ0RyiOCIUC2qu5W1QJgHjDOewFVzVO9cCtuIyDybss9vBSOr/umO4mqJCRD8pXwlV0nMMbUDX4FgYh0FJFR7vNEEUnyY7X2wH6v1znuvPLb/raIbAf+hXNU4Gv/k9xTR6tzc3P9KTl4tv2v051E54n+r5OSBV+vgrOHQ1eXMcYESZVBICL/DrwBPOfO6gD8w49ti495F33jV9W3VLUnMB74la8NqepsVR2kqoNatWrlx66D5MRmOLgIuk8FT4L/65WeQjq4ODR1GWNMEPlzRHAvMAw4BaCqO4HWfqyXA6R6ve4AHKhoYVVdDqSJSLIf264d2/8AnobQbXLVy3pr3h8S2tp1AmNMneBPEJx3z/EDICKx+HcufxXQTUQ6i0g8cDMw33sBEekqIuI+HwjEA8f8LT6kzhyAPa9A2l3QoGVg60qMc1RwcDGUFIWmPmOMCRJ/gmCZiPw/IFFERgOvA/+saiVVLQKmAIuBbcBrqrpFRCaLSOlX7O8Am0VkPU4Lo+95XTwOry+mO3cJ96igO4mqpGRB4Qk4+mlw6zLGmCCTqj53RSQGuBsYg3Pef7GqPl8Ltfk0aNAgXb16dWh3Unga/pEKbUfD1a9XbxsFJ+HNZOfeg/6/DW59xhgTIBFZo6qDfL3nzxHBVFV9XlVvUtUJqvq8iNwX5Bojy64XoPCkfzeQVSS+KbRKt+sExpiI508Q3OFj3g+CXEfkKClyupNodTUkX1GzbaVkwYmNTvfVxhgToSoMAhG5RUT+CXQWkfle01Ii5YJuKOx7A87sq9nRQKn21zuPdlRgjIlgsZW89wlwEEgG/uA1/zSwMZRFhY2qcwNZUndo/62ab69JL2jU0QmCrpNqvj1jjAmBCoNAVfcCe4Graq+cMDvyIRxfC0Oe8687iaqIOKeHvnwZis+XHePYGGMihD93Fl8pIqtEJE9ECkSkWERO1UZxtW7bE5DQGjrfHrxtpmRBUT7krgjeNo0xJoj8+do7A7gF2AkkAj8EpoeyqLA4scU5hdNtSmDdSVSlzXUQ08A6oTPGRCy/zn+oajbgUdViVX0RuDa0ZYXB9ifBkwjdfhTc7cY2hDbX2gVjY0zE8icIzrhdRKwXkd+LyAM4XUbXH2cPwp6/Qpc7nW6kgy0lC05/Aaezg79tY4ypIX+C4PvuclOAfJyO5L4TyqJq3Y7pznCUPR8IzfZLeyM9sDA02zfGmBqoNAjcUcZ+o6rnVPWUqj6uqj91TxXVD4V5sPNZSL0RkrqGZh9JadCkh50eMsZEpEqDQFWLgVbuqaH6afdcp3O4YNxAVpl2Wc5oZ0X5od2PMcYEqLIbykrtAT4Wkfk4p4YAUNUnQ1VUrbnQncQwZ3jJUGqfBTuecsIgGDerGWNMkPhzjeAAsMBdNslrqvv2vwn5e6BniI8GwOm7KLaRnR4yxkScKo8IVPXx2iik1l3oTqIbtP+30O/P0wDajnKCQNW569gYYyJAEPpRqKOOLIev10DPn0GMp3b2mXI95O+Fk1trZ3/GGOOH6A2CbU9Ag+TgdidRlZSxzqOdHjLGRJAqm4+6N5DVLye3wYEF0H0KxCbW3n4bdoBmfS0IjDERxZ/mo+NqqZbas/0PTn9C3X5c+/tOyYLcj5yhLI0xJgL4c2roYxGZISJXi8jA0inklYXK2UPw5V/c7iRa1f7+U64HLYJDS2p/38YY44M/9xEMdR9/6TVPgeuCX04t+GKG051EjzCd8Uq+EuKaOaeHLpkQnhqMMcaLP81H609Po0X5sPMZ6DAemnQLTw0xsdAuw+l3SEuCMwCOMcbUgD8D0zQVkSdFZLU7/UFEmtZGcUG3ay4UHA99dxJVScmCc4fg+Prw1mGMMfh3jWAuzjjF33WnU8CLoSwqJEq7k0i+CloNrXr5UErJBMRaDxljIoI/QZCmqo+q6m53ehzoEurCgi7nLcj/Enr9PKibXZS9iN8s/01gKyW0hpaDbdQyY0xE8CcIzopIeukLERkGnA1dSSHSKh36/Rba3xC0TZZoCfctuo//XvrfHD1zNLCVU7Lg2OdwLsD1jDEmyPwJgsnATBHZIyJ7cMYwviekVYVCYju49KGgdiexZNcSvjj2BYqyZFeAzUFTsgCFg4uDVo8xxlRHVXcWxwA9VLUf0Bfoq6oDVHVjrVQX4WasmkHrRq1pntCcxbsC/EBvcblzisiuExhjwqyqO4tLcIaoxB2h7FStVFUH7D6+m3998S/uufweRqeNZvGuxaiq/xuQGGg3Fg4ugpLi0BVqjDFV8OfU0BIReVBEUkWkRekU8soi3DOrnsET42HyoMlkpmVyKO8QGw8HeKCUkgUFXzvXCowxJkz8CYK7gHuB5cAad1odyqIiXX5BPi+se4Ebe91ISlIKY9LGAAR+eqjdaBCPnR4yxoSVP9cIJqpq53JT3Ws+GkT/t+n/OHHuBFOHTAWgfZP29GndJ/AgiG8OyUMtCIwxYeXPNYInaqmWOkFVmbFqBv3a9GNY6rAL8zPTMlmxdwV5BXmBbbD99XB8HZw5EORKjTHGP/6cGnpXRL4jYmMrAqzYt4KNhzcydchUvH8lGV0zKCwp5MM9Hwa2wZQs5/HgwuAVaYwxAfAnCH4KvA4UiMgpETktIlHbemjGyhk0T2jOLZfdUmZ++iXpNIxryKLsRYFtsGkfZ8AaOz1kjAmTKoNAVZNUNUZV41S1ifu6iT8bF5FMEdkhItki8pCP928TkY3u9ImI9KvOD1Fbck7l8Pdtf+fuAXfTMK5hmfcSYhO4ptM1gV8nEHGOCg4ugeKCIFZrjDH+8af3URGRiSLy3+7rVBEZ4sd6HmAmMBboDdwiIr3LLfYlMEJV+wK/AmYH+gPUpudWP0eJlvDjwb5HNstIyyD762x2fb0rsA2nZEHRaTj6cRCqNMaYwPhzaugZ4CrgVvd1Hs4HfFWGANluR3UFwDzKDXupqp+o6nH35WdAB7+qDoPzReeZvXY23+r+LTo37+xzmcyumUA1mpG2GQkx8dYJnTEmLPwJgitU9V7gHID7wR3vx3rtgf1er3PceRW5G/B5xVREJpWOh5Cbm+vHroPv9a2vcyT/yIUmo750a9GNTs06BR4EcY2h9Qi7TmCMCQt/gqDQPc2jACLSCijxYz1frYx89sEgItfiBMEvfL2vqrNVdZCqDmrVKgzjDAPTV06nR8sejOwyssJlRISMtAw++PIDCgI935+SBae2Qd6XNazUGGMC408Q/Al4C2gtIr8BPgL+x4/1coBUr9cdgIsay4tIX2AOME5Vj/mx3Vq38quVrPxqJfcOvpeYKoaWzOyaSV5BHp/s/ySwnZQ2Iz1gzUiNMbXLn1ZDrwD/AfwWOAiMV9XX/dj2KqCbiHQWkXjgZmC+9wIicgnwd+D7qvpFoMXXlpmrZtI4vjF39L+jymWv63wdsTGxLM4O8PRQk+7QuCscsOsExpja5dfI6aq6XVVnquoMVd3m5zpFOD2XLga2Aa+p6hYRmSwik93FHgFaAs+IyHoRibg+jI7kH2He5nnc0e8OmjSoutVskwZNGJo6lEW7AryfAJyjgsMfQFHdG/fHGFN3+RUE1aWq76hqd1VNU9XfuPNmqeos9/kPVbW5qvZ3p0GhrKc65qydQ0FxAVOGTPF7nYy0DNYfWs/hvMOB7SwlC4rPwZEPA1vPGGNqIKRBUNcVlRTx7OpnGdVlFD2Te/q9XkZaBgDv7no3sB22GQGehtZ6yBhTqywIKvH29rfJOZVTaZNRXwa0G0Crhq0CPz3kSYC2I537CQIZ5MYYY2rAgqAS01dOp2PTjlzf7fqA1ouRGMakjeHdXe9Sov60tPWSkgX5X8KpHYGtZ4wx1WRBUIFNhzexbO8yfjz4x3iqMeB9RloGR88cZd3BdYGtmDLWebTTQ8aYWmJBUIGZq2aSEJvA3QPurtb6paOWBdwbaaOO0PRSCwJjTK2xIPDh+Nnj/GXjX7i1z620bNiyWtto07gNA9oOCLy7CXBOD+Uuh8LT1dq3McYEwoLAh5fWv8SZwjMBNRn1JbNrJp/s/4ST504GtmLK9VBSCIfeq9H+jTHGHxYE5ZRoCTNXzWRY6jAGtBtQo21lpGVQrMV88OUHga3YaijENYGND0POP60FkTEmpCwIylmUvYhdx3cF3GTUl6tSr6JxfOPATw/FxMFVLzt3GC+/ARZdDvv/DoG2QDLGGD9YEJQzfeV02jVux7d7fbvG24r3xDOy80gWZS9CA/1W32Ec/NsOuPJFKMqDFd+Bd/rB3lehpLjGtRljTCkLAi87j+1kUfYi7rn8HuI9/gy5ULWMtAz2ntzLF8eq0adeTBx0+QFcvxWGvgJaDB/fDO9cCl/+BUqKglKjMSa6WRB4eWbVM8TFxHHPoHuCts2Mrk53E9VqPVQqJhY63QrXb4b015zRzD69HRb0hF1znQvLxhhTTRYErryCPOaun8uE3hNo27ht0LbbpXkXurXoFvj9BL5IDFxyE4xdD1e/BXFN4fO74Z/dYOcsKD5f830YY6KOBYHrrxv/yqnzp4Jykbi8jLQMPtzzIeeKzgVngxIDqeMhczWMWAAJbWHVj2B+GuyYbt1YG2MCYkEAqCozVs5gYLuBXNnhyqBvP7NrJmeLzrJi74rgblgE2l8PYz6Fa9+Fxp1hzU9gfhfY9iQU5Qd3f8aYesmCAPhwz4dsyd3ClMFTEPE11HLNXNPpGuI98TW7TlAZEWg3GkYth5FLoWlvWPczeLsTbJlmdygbYyplQYDTZLRlYktu7nNzSLbfKL4R6Zekhy4ISolAm2tg5Psw+iNocTls+E8nEDb9CgpOhHb/xpg6KeqDYN/Jfby9421+OPCHJMYlhmw/mWmZbD6ymZxTOSHbRxmthsG1i2DM55A8FDY9Am93hA3/Dee/rp0ajDF1QtQHwazVswD40aAfhXQ/pc1IAx61rKaSh8A1/4TMtdB2FGz5NczvDJseh4IA+0AyxtRLUR0E54rO8fza57mhxw10bNYxpPu6rPVltGvcLjjNSKujxQC4+k0YuwHaXAebHnMCYctvoTAvPDUZYyJCVAfBq5tf5eiZoyFpMlqeiJDRNYP3dr9HcTi7iGjeF4a/5TQ9Tb4KNvw/t5XRH6DoTPjqMsaETdQGgaoyfeV0eiX34tpO19bKPjPSMjh+7jirDqyqlf1VqsXlcM2/YPQn0LwfrHvwm/sQ7MY0Y6JK1AbB5199zpqDa5gyJDRNRn0Z3WU0goTv9JAvra6C65bAqGXQpLtzH8I/u8LO56C4INzVGWNqQdQGwfSV02nSoAm397u91vbZsmFLBrcfHPpmpNXRejiM/NAJhcQOsGoyLOgBu160zu2MqeeiMggO5R3i9S2v84N+P6BxfONa3XdGWgYrv1rJ12cjsAmniNOyaMwnMOJf0KAlfH4X/Ks3fPmKdX9tTD0VlUHw/JrnKSwp5N4h99b6vjO7ZlKiJby3O4KHoRSB9lmQscrp3M6TAJ9OhIV9Yd/rNkCOMfVM1AVBYXEhs9bMIiMtg+4tu9f6/oe0H0LTBk1ZnB2Bp4fKE3E6txu7Hoa96gyZ+dF3YeFAyHnbhtA0pp6IuiB4a/tbHDh9oMYD01dXbEwso9NGs2hXNUYtCxeJgY7fhaxNcNVfnM7slo+HxUPgwEILBGPquKgLgukrp9OleRfGdh0bthoy0jI4cPoAW3K3hK2GaonxQOeJ8K1tcMULcD4XPsyCJelOIBQcD3eFxphqiA13AbVp/aH1fLTvI54Y/QSeGE/Y6shIc0cty15Mn9Z9wlZHtcXEQtpd0Gki7J4Lm3/tBAJAQhto0hOa9HAf3anhJU6QGGMiTlQFwYyVM0iMTeSuAXeFtY7Upqn0btWbRbsW8bOhPwtrLTXiiYduk51xlQ+9Bye3wantzrTvDSjwahnlSYCk7heHRFJ3iKvdllvGmLKiJgi+Pvs1r2x6he/3/T7NE5uHuxwy0jJ4ZtUznCk8Q8O4huEup2Y8CdD+W87k7dzRb4KhdPp6Lex/o2zLo4apZY8eSsMiMcW5YG2MCamoCYJ/bP8H54rOhe0icXmZXTN56rOnWLZnGWO7he96RUglJENCOrROLzu/+Dyczr44JHa/BEVeg+jENoYWA6H9OOgwDpLSarV8Y6KF1JmWK65Bgwbp6tWrA15PVdl8ZDOXtbksBFUF7mzhWVr8vgWTBk7ij2P/GO5yIoMqnD3oFQ7b4MgKOLHBeb9pH+gw3mnS2nygHS0YEwARWaOqg3y9F9IjAhHJBP4IeIA5qjqt3Ps9gReBgcB/qeoTIawlYkIAIDEukREdR0RmdxPhIgINU5yp7XXfzM/b49y3kPMP2Po/zpgKDTs4Rwqp46H1CIiJC1vZxtR1IWs+KiIeYCYwFugN3CIivcst9jXwEyBkARDJMrtmsuPYDvac2BPuUiJb407Q8z4YtRS+fRiufAlaDHZaLH0wGt5sBR/f5tz1bOMzGxOwUN5HMATIVtXdqloAzAPGeS+gqkdUdRVQGMI6IpZ3M1Ljp4Rk6HIHDP87fOcoDH8bUm+EQ+86dz2/mQxLsyB7Npw9FO5qjakTQhkE7YH9Xq9z3HkBE5FJIrJaRFbn5uYGpbhI0DO5J6lNUu30UHXFNrwUuwwAABDiSURBVIQON8CVc+Hbh2DUcug+BU7vgJX3wFsp8O5Q2Po7OLUj3NUaE7FCGQS+ruRV68q0qs5W1UGqOqhVq1Y1LCtyiAiZXTN5b/d7FBZH5UFR8MR4oPXVMPAP8G/ZTncYfX8JJQWw/iFY0BMW9HKeH/3MelI1xksoLxbnAKlerzsAB0K4vzopIy2D59c+z2c5n3F1x6vDXU79IALN+jhTn4chfz98Nd+52LztD84RQkwDSOpW9t6Fpj0hqYfd4GaiTiiDYBXQTUQ6A18BNwO3hnB/ddLILiPxiIfFuxZbEIRKo1Tofq8zFRyHA4vg+DqnieqJjZDzFqjXEUJi+4sDoklPZ741WTX1UEjvIxCRLOBpnOajc1X1NyIyGUBVZ4lIW2A10AQoAfKA3qp6qqJtVvc+gkiWPjedc0XnWD2pfv1cdUZxAeTtuvgGt1PbodDrn2JsI6+uMXp8ExBJ3Zy7q42JYGG7j0BV3wHeKTdvltfzQzinjKJaRloGj3z4CEfyj9C6UetwlxN9PPHQtJczeVOFc4cvDofcj2DPK14LCjTu7PablAQx8c59DTHxziRxzj78fX7R+rGhORJp0No5WjJRL2q6mIhkmV0zeeTDR1iyawm39b0t3OWYUiKQ2NaZ2lxT9r2iM3D6Czi53WmldGo7nN4J+XudC9Qlhe5juefVay8ROo06OjfktR7hjFvdOM1Of0UhC4IIMLDdQFomtmTxrsUWBHVFbENo3t+Z/KXqXIuoKCTKPPd+HaIWZfl74MgyZyyJL1925iWmOIFQGg5NelowRAELggjgifEwOm007+56lxItIUaibryg6CDinOaJiQUSw12No8dUJ6BObXdC4chy53HvPOf9Bq3KBkOzPs6IdaZesSCIEJlpmczbPI8NhzYwoN2AcJdjoonIN9dIuk12giFv1zehcGQZ7H/TWTa+ObS6+ptTSc37u8Fm6jL7G4wQY9LGALB412ILAhNeIpDU1ZnS3EGc8vd6BcNy574MgNgkaJX+zVFDy0HWAWAdFDXdUNcF/Wf1p3lic5besTTcpRhTuTMHvgmG3OVwcqsz39PQ6Rk2tpE7Nfbx3Ne8Sp7bqaigCFvzUROYjLQMnvzsSU6fP01Sg6Rwl2NMxRqmQKebnQngXC7krnDGjzh3CIryoSjPGa70zD73tTuv+Fxg+/IkOoEQ06Bss9oyz8u/rs5yfm7D4zb3Lf88Ju7iC+uqoEW+GwQUF4AWBva8xQBoNSw4f4deLAgiSEbXDH7/ye9ZumcpN/S4IdzlGOO/hFZOL7CpN1a9bEkxFOeXDYeifCjM85rvY17ph2GJ++Ho/bzknHPzn/eHZomPD9Ti84S0CW9MnBMMIqFp8dXrPywI6rthqcNoFNeIRdmLLAhM/RXjgZgmENckPPsvKfYKiPPOh3X5YPH1vPw3+sqWQ6s4gqnO0Uycc8osBCwIIkiD2AZc2/la65bamFCK8QAep1sQu64NhLYbalMNmWmZ7D6+m+yvs8NdijEmSlgQRJiMrs6oZYuyF4W5EmNMtLAgiDBdW3SlS/MudnrIGFNrLAgiUGZaJku/XMr5ovPhLsUYEwUsCCJQRtcM8gvz+Xj/x+EuxRgTBazVUAS6ttO1xMbE8uiHj7Lv5D6GdxxO52adEesF0hgTAhYEESipQRKPjXiMpz57ijvfvhOADk06MKLjCEZ0HMHwjsPp3rK7BYMxJiisr6EIVqIlbM3dyvK9y1m2dxnL9izjcP5hANo2bsvwjsMvBEPvVr2t+2pjTIUq62vIgqAOUVW+OPbFN8Gwdxk5p3IAaJnYskww9G3TF0+MJ8wVG2MihQVBPaWq7Dmx50IoLNuzjC9PfAlA0wZNubrj1ReCYWC7gcRav/HGRC0Lgiiy/+T+MkcMXxz7AoDG8Y0ZljqMER1HcNeAu2jTuE2YKzXG1CYLgih2KO+QEwx7nGDYkruFpg2a8uvrfs3kQZPtKMGYKGFBYC7YcXQHUxdOZcnuJfRv25+ZWTMZmjo03GUZY0KssiCwZiZRpkdyDxZPXMzrN73O0TNHGTZ3GHe+fSdH8o+EuzRjTJhYEEQhEWFC7wlsu3cbvxj2C/668a/0mNGDmStnUlxSHO7yjDG1zIIgijWOb8y0UdPYOHkjl7e7nCkLpzD4+cF8uv/TcJdmjKlFFgSGXq16seT7S3h1wqsczj/M0LlDufvtu8nNzw13acaYWmBBYADndNF3L/0u2+/dzs+H/pyXN75Mjxk9eHbVs3a6yJh6zoLAlJHUIInfj/49GyZvoH/b/vz4nR9zxZwrWPnVynCXZowJEQsC41PvVr15//b3+dt3/saB0we4cs6VTPrnJI6eORru0owxQWZBYCokItzc52Z2TNnBT6/6KXPXzaXHjB7MXjPbThcZU49YEJgqJTVI4okxT7B+8noua30Z9yy4h6teuIpVX60Kd2nGmCCwIDB+69O6D0vvWMorN77C/lP7uWLOFdzzz3s4duZYuEszxtSABYEJiIhw62W3smPKDu6/8n5eWPcC3Wd050+f/4lP939KzqkcikqKwl2mMSYA1teQqZFNhzdx7zv3smLfigvzPOKhXVI7OjTpQGqT1LKPTZ3Hdo3b2XgJxtSisHU6JyKZwB8BDzBHVaeVe1/c97OAM8APVHVtZdu0IIg8qsqW3C3sO7mP/Sf3k3Mqh5zTORee7z+1nzOFZ8qsUxoW5YPCwsKY0KgsCELWB7GIeICZwGggB1glIvNVdavXYmOBbu50BfCs+2jqEBGhT+s+9Gndx+f7qsqJcyfYf8oNBq+AyDmVw4bDG1jwxQLOFp0ts55HPCQ3TKZRfCMaxzemUZz7GN/om+de8ypcxuu9RvGNrOttY8oJ5f+IIUC2qu4GEJF5wDjAOwjGAS+rc1jymYg0E5F2qnowhHWZWiYiNE9sTvPE5vRt09fnMqrK8XPHLwqK3Pxc8gvzySvII78wn9MFpzmUd+ibeQX55BfmB1RPA08D4jxxwfjRjKlVD1z5AL+89pdB324og6A9sN/rdQ4Xf9v3tUx7oEwQiMgkYJL7Mk9EdgS31BpLBurSnVZ1qd6g13re/RMiUf27DbG6VG9Iav2V+6eaOlb0RiiDQHzMK39Bwp9lUNXZwOxgFBUKIrK6onNvkagu1VuXaoW6VW9dqhXqVr11qVYIbfPRHCDV63UH4EA1ljHGGBNCoQyCVUA3EeksIvHAzcD8csvMB24Xx5XASbs+YIwxtStkp4ZUtUhEpgCLcZqPzlXVLSIy2X1/FvAOTtPRbJzmo3eGqp4Qi9jTVhWoS/XWpVqhbtVbl2qFulVvXaq17t1QZowxJrisiwljjIlyFgTGGBPlLAhqQERSRWSpiGwTkS0icl+4a6qKiHhEZJ2ILAh3LVVxbzB8Q0S2u7/jq8JdU0VE5AH338BmEfmbiCSEuyZvIjJXRI6IyGaveS1EZImI7HQfm4ezxlIV1Pq/7r+DjSLylog0C2eN3nzV6/XegyKiIpIcjtr8ZUFQM0XAz1S1F3AlcK+I9A5zTVW5D9gW7iL89Edgkar2BPoRoXWLSHvgJ8AgVe2D0zji5vBWdZGXgMxy8x4C3lfVbsD77utI8BIX17oE6KOqfYEvgP+s7aIq8RIX14uIpOJ0sbOvtgsKlAVBDajqwdJO8lT1NM4HVfvwVlUxEekAXA/MCXctVRGRJsBw4AUAVS1Q1RPhrapSsUCiiMQCDYmw+2FUdTnwdbnZ44A/u8//DIyv1aIq4KtWVX1XVUv7N/8M556jiFDB7xbgKeA/8HGTbKSxIAgSEekEDAA+D28llXoa5x9mSbgL8UMXIBd40T2VNUdEGoW7KF9U9SvgCZxvfgdx7od5N7xV+aVN6X077mPrMNfjr7uAheEuojIicgPwlapuCHct/rAgCAIRaQy8CdyvqqfCXY8vIvIt4Iiqrgl3LX6KBQYCz6rqACCfyDl1UYZ7bn0c0BlIARqJyMTwVlU/ich/4ZySfSXctVRERBoC/wU8Eu5a/GVBUEMiEocTAq+o6t/DXU8lhgE3iMgeYB5wnYj8NbwlVSoHyFHV0iOsN3CCIRKNAr5U1VxVLQT+DgwNc03+OCwi7QDcxyNhrqdSInIH8C3gNo3sG6DScL4UbHD/v3UA1opI27BWVQkLghpwB9Z5Adimqk+Gu57KqOp/qmoHVe2EcyHzA1WN2G+tqnoI2C8iPdxZIynbhXkk2QdcKSIN3X8TI4nQC9vlzAfucJ/fAbwdxloq5Q5y9QvgBlU9U9Xy4aSqm1S1tap2cv+/5QAD3X/TEcmCoGaGAd/H+Xa93p2ywl1UPTIVeEVENgL9gf8Jcz0+uUctbwBrgU04/68iqosBEfkb8CnQQ0RyRORuYBowWkR24rRumVbZNmpLBbXOAJKAJe7/s1lhLdJLBfXWKdbFhDHGRDk7IjDGmChnQWCMMVHOgsAYY6KcBYExxkQ5CwJjjIlyFgQm4ohIJ189OQZx++O9OwcUkV+KyKggbfsdt9fUZiLy42Bs02vb97t3rZbZVzD3YaKTNR81Ecftt2mB25NnKLb/krv9N0KxfXcfnQjwZ3BvRhNV9dkXlHuX6iBVPRqMGo0pZUcEJlLFisif3f7n3yj9JiwiI91O6Da5/cA3qGL+NBHZ6m7nCREZCtwA/K97Y1KaiLwkIhPc5feIyOMistbdVk93fiu3z/61IvKciOz11ce8u34yzs1Zae4+/td97+cissqt5XF3Xidxxlp4BueGtFQReVZEVoszvkHpcj/B6cdoqYgsLbcvROSn4oyFsFlE7i+37efdbb0rIoml2/P6vcwLxV+gqUNU1SabImoCOuF03TvMfT0XeBBIAPYD3d35LwP3VzK/BbCDb458m7mPLwETvPZ34TWwB5jqPv8xMMd9PgP4T/d5pltfso/a9wDJ7s+w2Wv+GJy7jQXnC9gCnG62O+H0Bnul17It3EcP8CHQ13vbPvZ1Oc4dzY2AxsAWnJ5wO+F00NbfXf41YKL7/ADQwPv3YlP0TnZEYCLVflX92H3+VyAd6IHTudsX7vw/43yYVjT/FHAOmCMiNwL+9lFT2nngGpwPU9z9zwNQ1UXA8QB/njHutA7nm39PoJv73l5V/cxr2e+KyFp32UuBqgY7SgfeUtV8Vc1z67/afe9LVV3v4+fZiNN9x0ScsDBRzILARKryF68U59u0Lz7nqzOQyRCc3mHHA4v83Pd597EYpzvsCvcRAAF+q6r93amrqr7gvpd/YSGRzjhHPyPVGY3rXzhHPFVtuyLnvZ57/zzXAzNxjibWiDOgjolSFgQmUl0i34xRfAvwEbAd6CQiXd353weWVTRfnHEimqrqOzinivq775/G6cAsEB8B3wUQkTFAVeP7lt/HYuAutyZEpL2I+BoIpglOMJwUkTbA2Eq2WWo5MN7t/bQR8G1gRUWFiUgMkKqqS3EGKmqGc0rJRCn7FmAi1TbgDhF5DtiJM0DNORG5E3jd/Qa7Cpilqud9zce5RvC2OAPJC/CAu+15wPPuBdgJftbzOPA3EfkeTvgcxPlg9klVj4nIx24z2IWq+nMR6QV86jQOIg+YiPMt3Xu9DSKyDuc8/27gY6+3ZwMLReSgql7rtc5atyXUSnfWHFVd57Zc8sUD/FVEmuL8Xp7SyB4G1ISYNR81xg9uK6RiVS1yj1SeVdX+Va1nTF1gRwTG+OcS4DX3tEoB8O9hrseYoLEjAmOMiXJ2sdgYY6KcBYExxkQ5CwJjjIlyFgTGGBPlLAiMMSbK/X9rHZ9HAFH7NgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators= 15\n",
      "Acuracy (on test set) =  0.9299221357063404\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98        87\n",
      "           1       0.88      0.93      0.91        89\n",
      "           2       0.95      0.94      0.95        84\n",
      "           3       0.97      0.84      0.90        92\n",
      "           4       0.93      0.93      0.93        88\n",
      "           5       0.93      0.95      0.94        94\n",
      "           6       0.98      0.98      0.98        94\n",
      "           7       0.95      0.94      0.94        99\n",
      "           8       0.83      0.90      0.86        84\n",
      "           9       0.89      0.92      0.91        88\n",
      "\n",
      "    accuracy                           0.93       899\n",
      "   macro avg       0.93      0.93      0.93       899\n",
      "weighted avg       0.93      0.93      0.93       899\n",
      "\n",
      "\n",
      " CONFUSION MATRIX\n",
      "[[84  0  0  1  1  0  0  0  1  0]\n",
      " [ 0 83  0  0  0  1  0  1  3  1]\n",
      " [ 0  2 79  0  1  0  0  0  1  1]\n",
      " [ 0  1  4 77  0  4  1  0  3  2]\n",
      " [ 0  2  0  0 82  0  0  3  1  0]\n",
      " [ 0  0  0  0  0 89  1  0  1  3]\n",
      " [ 0  0  0  0  0  0 92  0  2  0]\n",
      " [ 0  1  0  0  2  0  0 93  1  2]\n",
      " [ 0  4  0  0  1  1  0  1 76  1]\n",
      " [ 0  1  0  1  1  1  0  0  3 81]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Create and train an adaBoost classifier using SMALL Decision Trees as weak classifiers\n",
    "weak_learner = tree.DecisionTreeClassifier(max_depth=6)\n",
    "clf = AdaBoostClassifier(weak_learner, n_estimators=15, learning_rate=1.0, algorithm='SAMME', \n",
    "                         random_state=None)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "print(\"Weak_learner:\", clf.base_estimator)\n",
    "print(\"Weights of weak classifiers: \", clf.estimator_weights_)\n",
    "      \n",
    "# Plot training curves (error = f(iterations))\n",
    "n_iter = clf.n_estimators\n",
    "from sklearn.metrics import zero_one_loss\n",
    "ada_train_err = np.zeros((clf.n_estimators,))\n",
    "for i, y_pred in enumerate(clf.staged_predict(X_train)):\n",
    "    ada_train_err[i] = zero_one_loss(y_pred, y_train)\n",
    "ada_test_err = np.zeros((clf.n_estimators,))\n",
    "for i, y_pred in enumerate(clf.staged_predict(X_test)):\n",
    "    ada_test_err[i] = zero_one_loss(y_pred, y_test)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(np.arange(n_iter) + 1, ada_train_err,\n",
    "        label='Training Error',\n",
    "        color='green')\n",
    "ax.plot(np.arange(n_iter) + 1, ada_test_err,\n",
    "        label='Test Error',\n",
    "        color='orange')\n",
    "ax.set_ylim((0.0, 0.5))\n",
    "ax.set_xlabel('boosting iterations')\n",
    "ax.set_ylabel('error rate')\n",
    "leg = ax.legend(loc='upper right', fancybox=True)\n",
    "plt.show()\n",
    "\n",
    "# Evaluate acuracy on test data\n",
    "print(\"n_estimators=\", clf.n_estimators)\n",
    "score = clf.score(X_test, y_test)\n",
    "print(\"Acuracy (on test set) = \", score)\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print( classification_report(y_true, y_pred) )\n",
    "print(\"\\n CONFUSION MATRIX\")\n",
    "print( confusion_matrix(y_true, y_pred) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
